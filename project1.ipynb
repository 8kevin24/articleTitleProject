{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "API_KEY = '7fd6cc57-2121-4d82-ab66-548f409faf19'\n",
    "BASE_URL = 'https://content.guardianapis.com/search'\n",
    "\n",
    "# Parameters for the API request\n",
    "params = {\n",
    "    'api-key': API_KEY,\n",
    "    'from-date': '1989-01-01',\n",
    "    'to-date': '2024-06-30',\n",
    "    'page-size': 50,\n",
    "    'show-fields': 'headline',\n",
    "    'order-by': 'oldest',\n",
    "}\n",
    "\n",
    "def fetch_titles(page):\n",
    "    params['page'] = page\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data['response']['results']\n",
    "    else:\n",
    "        print(f'Failed to fetch page {page}: {response.status_code}')\n",
    "        return []\n",
    "\n",
    "titles = []\n",
    "page = 1\n",
    "while True:\n",
    "    results = fetch_titles(page)\n",
    "    if not results:\n",
    "        break\n",
    "    for result in results:\n",
    "        titles.append(result['fields']['headline'])\n",
    "    print(f'Fetched page {page}')\n",
    "    page += 1\n",
    "    time.sleep(0.1)  # to avoid hitting rate limits\n",
    "\n",
    "# Save titles to a CSV file\n",
    "df = pd.DataFrame(titles, columns=['Title'])\n",
    "df.to_csv('guardian_titles.csv', index=False)\n",
    "print('Saved titles to guardian_titles.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: Load data from a CSV file\n",
    "data = pd.read_csv('guardian_titles.csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing:\n",
    "\n",
    "Here we remove common stopwords like \"the\", \"a\", etc. While the presence of these might actually indicate relative success of an article title, we're looking for other keywords and similarities titles share. We'll also ignore tenses through \"Lemmatization\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_title(title):\n",
    "    tokens = word_tokenize(title.lower())\n",
    "    filtered_tokens = [lemmatizer.lemmatize(w) for w in tokens if not w in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "data['processed_title'] = data['Title'].apply(preprocess_title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix created successfully\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame (replace with your actual data)\n",
    "data = pd.DataFrame({\n",
    "    'Title': [\"This is a test title\", \"Another example title\", \"More data to process\", \"\", \"   \"]\n",
    "})\n",
    "\n",
    "# Preprocessing steps\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_title(title):\n",
    "    tokens = word_tokenize(title.lower())\n",
    "    filtered_tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Assuming 'data' is your DataFrame containing titles\n",
    "data['processed_title'] = data['Title'].apply(preprocess_title)\n",
    "\n",
    "# Filter out empty or whitespace-only titles after preprocessing\n",
    "data['processed_title_str'] = data['processed_title'].apply(lambda x: ' '.join(x))\n",
    "data = data[data['processed_title_str'].str.strip() != '']\n",
    "\n",
    "# Create TF-IDF matrix\n",
    "if not data.empty:\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "    tfidf_matrix = vectorizer.fit_transform(data['processed_title_str'])\n",
    "    print(\"TF-IDF matrix created successfully\")\n",
    "else:\n",
    "    print(\"No valid titles to process\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def compute_similarity(input_title):\n",
    "    # Preprocess the input title\n",
    "    preprocessed_input = ' '.join(preprocess_title(input_title))\n",
    "    preprocessed_input = ' '.join(preprocess_title(input_title))\n",
    "\n",
    "    # Transform the input title to TF-IDF vector\n",
    "    input_vector = vectorizer.transform([preprocessed_input])\n",
    "    print(input_vector.todense())\n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(input_vector, tfidf_matrix)\n",
    "    # Get the maximum similarity score\n",
    "    max_similarity = np.max(similarities)\n",
    "    return max_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(input_title):\n",
    "    similarity = compute_similarity(input_title)\n",
    "    # Scale similarity to 0-100\n",
    "    score = similarity * 100\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of usage of the similarity_score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "input_title = \"president\"\n",
    "print(f\"Similarity Score: {score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
